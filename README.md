<h1 align="center">Hello, bigData enthusiast! <img src="https://raw.githubusercontent.com/MartinHeinz/MartinHeinz/master/wave.gif" width="30px">
 
</div>

In data science, data is called "big" if it cannot fit into the memory of a single standard laptop or workstation.
Apache Spark is an indispensable data processing framework that everyone should know when dealing with big data.
When we try to perform data analysis on big data, we might encounter a problem that your current computer cannot cater
the need to process big data due to a limited processing power and memory resources in a single computer.
While we can try to upgrade our computer to meet the need of big data processing but we will soon find the 
computer can easily reach its maximum capacity again when dealing with the ever increasing datasets.


This repo is all about Programming Spark using Pyspark. Contribute to analyze large datasets using Jupyter, colaboratory, and Spark as a platform with a simple python code! please enjoy!

 <i>Loved the project? Please consider feel free to connect with me on [![LinkedIn][1.2]][1] to help it improve!</i>

<!-- Icons -->
[1.2]: https://raw.githubusercontent.com/MartinHeinz/MartinHeinz/master/linkedin-3-16.png (LinkedIn icon without padding)

<!-- Links to your social media accounts -->
[1]: https://www.linkedin.com/in/rifqijundullah/

### Contents:
- [Collabolatory](https://github.com/rifqij/bigData/tree/main/GoogleCollab)
- [DataBricks](https://github.com/rifqij/bigData/tree/main/DataBricks)
