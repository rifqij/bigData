# bigData

In data science, data is called "big" if it cannot fit into the memory of a single standard laptop or workstation.
Apache Spark is an indispensable data processing framework that everyone should know when dealing with big data.
When we try to perform data analysis on big data, we might encounter a problem that your current computer cannot cater
the need to process big data due to a limited processing power and memory resources in a single computer.
While we can try to upgrade our computer to meet the need of big data processing but we will soon find the 
computer can easily reach its maximum capacity again when dealing with the ever increasing datasets.
contribute to analyze large datasets using Jupyter, colaboratory, and Spark as a platform with a simple python code!

This repo is all about Programming Spark using Pyspark, please enjoy!
